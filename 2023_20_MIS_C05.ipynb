{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIS: Clase Complementaria 5 (2023-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargue de Paquetes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import itertools as it\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import graph_tools\n",
    "import random\n",
    "#import scipy.linalg as linalg \n",
    "#py -m pip install scipy==1.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Generación de Recompensas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera la función de recompensas\n",
    "# List_Distributions tiene dentro la información de cada uno de los brazos del bandido multibrazo con len(List_Distributions) brazos\n",
    "# Reps es la cantidad de veces que se jugará el bandido multibrazo\n",
    " \n",
    "def Generador_Recompensas(List_Distributions:list,reps:int):\n",
    "    Recompensas = {}\n",
    "    contador = 0\n",
    "    for i in List_Distributions:\n",
    "        if \"Bernoulli\" in i:\n",
    "            p = float(i.replace(\"Bernoulli\",\"\").replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "            Recompensas[contador] = [np.random.binomial(1,p) for j in range(reps)]\n",
    "        elif \"Binomial\" in i:\n",
    "            n,p = i.replace(\"Binomial\",\"\").replace(\"(\",\"\").replace(\")\",\"\").split(\",\")\n",
    "            Recompensas[contador] =  [np.random.binomial(n,p) for j in range(reps)]\n",
    "        elif \"Uniform\" in i:\n",
    "            a,b = i.replace(\"Normal\",\"\").replace(\"(\",\"\").replace(\")\",\"\").split(\",\")\n",
    "            Recompensas[contador] =  [np.random.uniform(low=a,high=b) for j in range(reps)]\n",
    "        elif \"Normal\" in i:\n",
    "            mu,sigma=i.replace(\"Normal\",\"\").replace(\"(\",\"\").replace(\")\",\"\").split(\",\")\n",
    "            Recompensas[contador] =  [np.random.normal(loc=mu,scale=sigma) for j in range(reps)]\n",
    "        else:\n",
    "            raise Exception(\"Introduzca una distribución válida.\")\n",
    "        contador += 1\n",
    "    return Recompensas\n",
    "\n",
    "\n",
    "# Más adelante se generarán las simulaciones. Por ahora nos concentraremos en armar los otros componentes de la práctica de hoy.\n",
    "# Además, las distribuciones anteriores son generales, pero nos centraremos en la Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandidos tipo Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializador de los estimadores Plug-In Q y de Priors de tipo Beta(\\alpha,\\beta)\n",
    "# Para Plug-In tipo Q se introducen listas de números. Para Priors tipo Beta se introducen listas de listas de 2 componentes\n",
    "\n",
    "def Inicializador_Input_Optimizacion(List_Distributions:list,List_Parameters:list,tipo:str):\n",
    "    l_dist = len(List_Distributions)\n",
    "    l_param = len(List_Parameters)\n",
    "    for parameter in List_Parameters:\n",
    "        if tipo==\"Q\":\n",
    "            if isinstance(parameter,(float,int))==False:\n",
    "                raise Exception(\"Existen parámetros que no corresponden con su elección de tipo Q\")\n",
    "        elif tipo==\"Beta\":\n",
    "            if (isinstance(parameter,(list))==False) or (len(parameter)!=2):\n",
    "                raise Exception(\"Existen parámetros que no corresponden con su elección de tipo Beta\")\n",
    "            else:\n",
    "                if any([((isinstance(k,(int,float))==False) or (k<=0)) for k in parameter]):\n",
    "                    raise Exception(\"Parametros inválidos para la distribución Beta\")\n",
    "        else:\n",
    "            raise Exception('Introduzca un tipo de inicialización válida')\n",
    "    if l_dist==l_param:\n",
    "        return List_Parameters\n",
    "    elif l_param==1:\n",
    "        return List_Parameters*l_dist\n",
    "    else:\n",
    "        raise Exception(\"Introduzca una listado de parámetros válidos. La lista que introdujo no puede ser procesada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la función de ArgMax necesaria en los modelos Greedy y Epsilon-Greedy\n",
    "\n",
    "def Optimizador_Arg_Max(List_Expected_Values:list,tipo:str=\"Q\"):\n",
    "    if len(List_Expected_Values)==0:\n",
    "        raise Exception(\"Introduzca una lista no vacía\")\n",
    "    if (isinstance(tipo,(str))==False) or (tipo not in ['Q',\"Beta\"]):\n",
    "        raise Exception(\"Introduzca un tipo válido\")\n",
    "    \n",
    "    index = [0]\n",
    "    l = len(List_Expected_Values)\n",
    "    contador = 0\n",
    "    if tipo==\"Q\":\n",
    "        max = List_Expected_Values[0]\n",
    "    elif tipo==\"Beta\":\n",
    "        alpha,beta = List_Expected_Values[contador]\n",
    "        max = alpha/(alpha+beta)\n",
    "    \n",
    "    while contador<l:\n",
    "\n",
    "        if tipo==\"Q\":\n",
    "            evaluado = List_Expected_Values[contador]\n",
    "        elif tipo==\"Beta\":\n",
    "            alpha,beta = List_Expected_Values[contador]\n",
    "            evaluado = alpha/(alpha+beta)\n",
    "\n",
    "        if evaluado>=max:\n",
    "            if evaluado==max:\n",
    "                index.append(contador)\n",
    "            else:\n",
    "                max=evaluado\n",
    "                index = [contador]\n",
    "        contador+=1\n",
    "    return random.choice(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la función de selección aleatoria de brazos (K el número de brazos). Se usa distribución uniforme\n",
    "\n",
    "def Selector_Aleatorio_Brazos(K:int):\n",
    "    return random.randint(0,K-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la función de \"lanzar la moneda\" para explorar/explotar\n",
    "\n",
    "def Explore_Exploit(epsilon:float):\n",
    "    return np.random.binomial(1,epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la función que permite ver la realización de la recompensa\n",
    "\n",
    "def Realizacion_Recompensa(Recompensas:dict,time:int,choice:int):\n",
    "    return Recompensas[choice][time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la función de sampleo de TS\n",
    "\n",
    "def Sampling(List_Parameters):\n",
    "    Sample = []\n",
    "    for i in List_Parameters:\n",
    "        alpha,beta=i\n",
    "        Sample.append(np.random.beta(alpha,beta))\n",
    "    return Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualización de las Q's y parámetros de las Betas según sea el caso\n",
    "\n",
    "def Actualizador(List_Parameters:list,reward:float,argmax:int,tipo:str,selected:list=[0]):\n",
    "    if tipo==\"Q\":\n",
    "        if len(selected)!=len(List_Parameters):\n",
    "            raise Exception(\"El record de elecciones de acciones está mal\")\n",
    "        else:\n",
    "            Q_present = List_Parameters[argmax]\n",
    "            if selected[argmax] == 0:\n",
    "                Q_new = 1\n",
    "            else:\n",
    "                Q_new = Q_present+(1/selected[argmax])*(reward-Q_present)\n",
    "            List_Parameters[argmax] = Q_new\n",
    "            selected[argmax] += 1\n",
    "    elif tipo==\"Beta\":\n",
    "        alpha_present,beta_present=List_Parameters[argmax]\n",
    "        List_Parameters[argmax] = [alpha_present+reward,beta_present+1-reward]\n",
    "    return List_Parameters,selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de tipo Q (Básicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QGreedy\n",
    "def QGreedy(Recompensas,Priors_t_0):\n",
    "    acciones = []\n",
    "    pagos = []\n",
    "    K = len(Recompensas)\n",
    "    T = len(Recompensas[0])\n",
    "    t=0\n",
    "    # Selected (número de veces que se ha seleccionado una acción)\n",
    "    selected = [0]*K \n",
    "    # Estimación del modelo\n",
    "    Q = Priors_t_0\n",
    "    while t<T:\n",
    "        # Elección de la acción\n",
    "        choice = Optimizador_Arg_Max(Q,\"Q\")\n",
    "        # Realización de la recompensa\n",
    "        reward = Realizacion_Recompensa(Recompensas=Recompensas,time=t,choice=choice)\n",
    "        # Guardado de las cosas relevantes\n",
    "        acciones.append(choice)\n",
    "        pagos.append(reward)\n",
    "        # Recálculo del modelo\n",
    "        Q,selected = Actualizador(List_Parameters=Q,reward=reward,argmax=choice,tipo=\"Q\",selected=selected)\n",
    "        print(choice,reward)\n",
    "        # Paso siguiente\n",
    "        t+=1\n",
    "\n",
    "    return acciones,pagos   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QEpsilonGreedy\n",
    "def QEpsilonGreedy(Recompensas,Priors_t_0,epsilon):\n",
    "    if (isinstance(epsilon,(float))==False) or (0<=epsilon<=1)==False:\n",
    "        raise Exception('Introduzca epsilon válido')\n",
    "    \n",
    "    acciones = []\n",
    "    pagos = []\n",
    "    K = len(Recompensas)\n",
    "    T = len(Recompensas[0])\n",
    "    t=0\n",
    "    # Selected (número de veces que se ha seleccionado una acción)\n",
    "    selected = [0]*K \n",
    "    # Estimación del modelo\n",
    "    Q = Priors_t_0\n",
    "    while t<T:\n",
    "        # Elección de la acción\n",
    "        if Explore_Exploit(epsilon=epsilon) == 1:\n",
    "            choice = Selector_Aleatorio_Brazos(K=K)\n",
    "        else:\n",
    "            choice = Optimizador_Arg_Max(Q,\"Q\")\n",
    "        # Realización de la recompensa\n",
    "        reward = Realizacion_Recompensa(Recompensas=Recompensas,time=t,choice=choice)\n",
    "        # Guardado de las cosas relevantes\n",
    "        acciones.append(choice)\n",
    "        pagos.append(reward)\n",
    "        # Recálculo del modelo\n",
    "        Q,selected = Actualizador(List_Parameters=Q,reward=reward,argmax=choice,tipo=\"Q\",selected=selected)\n",
    "        print(choice,reward)\n",
    "        # Paso siguiente\n",
    "        t+=1\n",
    "\n",
    "    return acciones,pagos   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de tipo Bayesiano con distribuciones Beta y uso de medias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BetaGreedy\n",
    "def BetaGreedy(Recompensas,Priors_t_0):\n",
    "    acciones = []\n",
    "    pagos = []\n",
    "    T = len(Recompensas[0])\n",
    "    t=0\n",
    "    # Estimación del modelo\n",
    "    Parameters = Priors_t_0\n",
    "    while t<T:\n",
    "        # Elección de la acción\n",
    "        choice = Optimizador_Arg_Max(Parameters,\"Beta\")\n",
    "        # Realización de la recompensa\n",
    "        reward = Realizacion_Recompensa(Recompensas=Recompensas,time=t,choice=choice)\n",
    "        # Guardado de las cosas relevantes\n",
    "        acciones.append(choice)\n",
    "        pagos.append(reward)\n",
    "        # Recálculo del modelo\n",
    "        Parameters,selected = Actualizador(List_Parameters=Parameters,reward=reward,argmax=choice,tipo=\"Beta\")\n",
    "        print(choice,reward)\n",
    "        # Paso siguiente\n",
    "        t+=1\n",
    "\n",
    "    return acciones,pagos   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BetaEpsilonGreedy\n",
    "def BetaEpsilonGreedy(Recompensas,Priors_t_0,epsilon):\n",
    "    if (isinstance(epsilon,(float))==False) or (0<=epsilon<=1)==False:\n",
    "        raise Exception('Introduzca epsilon válido')\n",
    "    \n",
    "    acciones = []\n",
    "    pagos = []\n",
    "    K = len(Recompensas)\n",
    "    T = len(Recompensas[0])\n",
    "    t=0\n",
    "    # Estimación del modelo\n",
    "    Parameters = Priors_t_0\n",
    "    while t<T:\n",
    "        # Elección de la acción\n",
    "        if Explore_Exploit(epsilon=epsilon) == 1:\n",
    "            choice = Selector_Aleatorio_Brazos(K=K)\n",
    "        else:\n",
    "            choice = Optimizador_Arg_Max(Parameters,\"Beta\")\n",
    "        # Realización de la recompensa\n",
    "        reward = Realizacion_Recompensa(Recompensas=Recompensas,time=t,choice=choice)\n",
    "        # Guardado de las cosas relevantes\n",
    "        acciones.append(choice)\n",
    "        pagos.append(reward)\n",
    "        # Recálculo del modelo\n",
    "        Parameters,selected = Actualizador(List_Parameters=Parameters,reward=reward,argmax=choice,tipo=\"Beta\")\n",
    "        print(choice,reward)\n",
    "        # Paso siguiente\n",
    "        t+=1\n",
    "\n",
    "    return acciones,pagos   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo de tipo bayesiano con distribuciones Beta y Thompson Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de tipo bayesiano con distribuciones Beta y Thompson Sampling \n",
    "def BetaTS(Recompensas,Priors_t_0):\n",
    "    acciones = []\n",
    "    pagos = []\n",
    "    T = len(Recompensas[0])\n",
    "    t=0\n",
    "    # Estimación del modelo\n",
    "    Parameters = Priors_t_0\n",
    "    while t<T:\n",
    "        # Sampleo\n",
    "        Sampleo = Sampling(List_Parameters = Parameters)\n",
    "        # Elección de la acción\n",
    "        choice = Optimizador_Arg_Max(Sampleo)\n",
    "        # Realización de la recompensa\n",
    "        reward = Realizacion_Recompensa(Recompensas=Recompensas,time=t,choice=choice)\n",
    "        # Guardado de las cosas relevantes\n",
    "        acciones.append(choice)\n",
    "        pagos.append(reward)\n",
    "        # Recálculo del modelo\n",
    "        Parameters,selected = Actualizador(List_Parameters=Parameters,reward=reward,argmax=choice,tipo=\"Beta\")\n",
    "        print(choice,reward)\n",
    "        # Paso siguiente\n",
    "        t+=1\n",
    "\n",
    "    return acciones,pagos  \n",
    "\n",
    "# Ojo, no tiene sentido proponer un Thompson Sampling Greedy y Epsilon Greedy, pues precisamente el sampling es lo que permite la exploración."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
